# -*- coding: utf-8 -*-
"""Blood_donation_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mMOPDOze-mkaDtmKOd4wX_nicOjB34_H

##1. Importing and Loading the data
"""
import warnings

warnings.filterwarnings('ignore')

import pandas as pd

#transfusion_data = pd.read_csv('/content/drive/MyDrive/Datasets/transfusion.data')
filepath = 'C:\\Users\\Devika\\Documents\\New folder\\transfusion.data'

# above .data file is comma delimited
transfusion_data = pd.read_csv(filepath, delimiter=",")

transfusion_data.head()

transfusion_data.info()

transfusion_data.describe()

{column: len(transfusion_data[column].unique())for column in transfusion_data.columns}

transfusion_data.shape

transfusion_data.isna().sum()

"""##2.Creating the target data and Renaming the columns

"""

transfusion_data.rename(
    columns={'whether he/she donated blood in March 2007': 'target', 'Recency (months)':'Recency',
             'Frequency (times)': 'Frequency','Monetary (c.c. blood)':'Monetary',	'Time (months)':'Time'},
    inplace=True
)

transfusion_data.head()

"""##3. Checking target incidence"""

data = transfusion_data.drop(columns="target")
target = transfusion_data["target"]

transfusion_data.target.value_counts(normalize=True).round(3)

transfusion_data.groupby("target").agg({"Recency":"mean"})

transfusion_data.groupby("target").agg({"Frequency":"mean"})

transfusion_data.groupby("target").agg({"Monetary":"mean"})

"""##4. Exploratory Data Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

g = sns.heatmap(transfusion_data[["target", "Recency","Frequency","Time"]].corr(),annot=True, fmt = ".2f", cmap = "coolwarm")

"""No of times they have donated since first donation have a good correlation with the target probability.

It doesn't mean that the other features are not usefull. Frequency features can be correlated with the target. To determine this, we need to explore in detail these features.

"""

_ = data.hist(figsize=(12, 10), bins=30, edgecolor="black")

target.value_counts(normalize=True).plot.barh()
plt.xlabel("Number of samples")
plt.ylabel("0 - Not Donated \n 1 - Donated")
_ = plt.title("Class distribution")

_ = sns.pairplot(transfusion_data, hue="target")

"""From this plot, we can see that frequency and monetary is highl correlated. so, we can use only the frequency.

##5. Splitting transfusion_data into train and test datasets
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,target, test_size=0.25,
                                                  random_state=42,stratify=transfusion_data.target)
#X_train, X_test, y_train, y_test = train_test_split(data,target,test_size=0.3,random_state=0,stratify=transfusion_data.target)
X_train.head(4)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit_transform(X_train)

X_train = pd.DataFrame(scaler.transform(X_train),index=X_train.index , columns=X_train.columns)
X_test = pd.DataFrame(scaler.transform(X_test),index=X_test.index, columns=X_test.columns)

"""# 6. Training a model"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier

from sklearn import svm
classifier1 = svm.SVC(kernel='linear')
classifier1.fit(X_train,y_train)

from sklearn.ensemble import RandomForestClassifier
classifier2 = RandomForestClassifier(n_estimators=50)
classifier2.fit(X_train, y_train)

from sklearn.linear_model import LogisticRegression
classifier3=LogisticRegression()
classifier3.fit(X_train, y_train)

from sklearn.tree import DecisionTreeClassifier
classifier4 = DecisionTreeClassifier()
classifier4 = classifier4.fit(X_train,y_train)

"""# 7. Model Evaluation"""

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

"""**SVM**"""

y_pred1 = classifier1.predict(X_test)
result1 = accuracy_score(y_pred1, y_test)
print("Accuracy:",result1)
print(classification_report(y_test,y_pred1))

"""**Random Forest**"""

y_pred2 = classifier2.predict(X_test)

result2 = accuracy_score(y_test,y_pred2)
print("Accuracy:",result2)
print(classification_report(y_test,y_pred2))

"""**Logistic Regression**"""

y_pred3 = classifier3.predict(X_test)

result3 = accuracy_score(y_test,y_pred3)
print("Accuracy:",result3)
print(classification_report(y_test,y_pred3))

"""**Decision Tree**"""

y_pred4 = classifier4.predict(X_test)

result4 = accuracy_score(y_test,y_pred4)
print("Accuracy:",result4)
print(classification_report(y_test,y_pred4))

"""**Accuracies of each model**"""

print("SVM                 :", "{:.2f}%".format(result1*100))
print("Random Forest       :", "{:.2f}%".format(result2*100))
print("Logistic Regression :", "{:.2f}%".format(result3*100))
print("Decision Tree       :", "{:.2f}%".format(result4*100))

"""From the above accuracies, we can see the models that has high accuracy is Logistic regression.

Furthur, fine tuning the model and doing prediction on it will provide the high accuracy.

Logistic Regresion
"""
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

logreg = LogisticRegression(solver='saga',penalty='l2',C=25.0,random_state=42, max_iter=1000)
#logreg = LogisticRegression(C=25.0, random_state=42)
logreg.fit(X_train, y_train)

prediction = logreg.predict(X_test)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

acc = accuracy_score(y_test, prediction)
print("Accuracy:",acc)

#Confusion matrix
confusion_matrix(prediction,y_test)

print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(logreg.score(X_train, y_train)))
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

pred_report = pd.DataFrame(prediction.tolist(),columns=["target"])
# saving the prediction
pred_report.to_csv("final_submission.csv")

import pickle

# Saving model to disk
pickle.dump(logreg, open('model.pkl','wb'))


"""Now we can approach the people who are interested in donating blood and which will results in getting more volunteers and we can save more people."""
